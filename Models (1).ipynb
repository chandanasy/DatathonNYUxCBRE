{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "p6iBeUq1F_rG"
      },
      "outputs": [],
      "source": [
        "#import required libraries\n",
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import zipfile\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#connect to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGlj_PKTGEmp",
        "outputId": "08640694-a0ee-48ad-926d-e79598b1c4ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Class to preprocess images\n",
        "class ConstructionSiteDataset(Dataset):\n",
        "    def __init__(self, base_dir, transform=None, device='cpu'):\n",
        "        self.base_dir = base_dir\n",
        "        self.transform = transform\n",
        "        self.device = device\n",
        "        self.classes = ['underdeveloped', 'ground_broken', 'concrete_pad', 'framing_up', 'near_completion']\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        self.images = []\n",
        "        self.class_counts = {cls_name: 0 for cls_name in self.classes}\n",
        "\n",
        "        for _class in self.classes:\n",
        "            class_dir = os.path.join(base_dir, _class)\n",
        "            class_images = [img_name for img_name in os.listdir(class_dir) if img_name.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
        "            self.images.extend([(os.path.join(class_dir, img), _class) for img in class_images])\n",
        "            self.class_counts[_class] += len(class_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, _class = self.images[idx]\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.class_to_idx[_class]\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def get_class_counts(self):\n",
        "        return self.class_counts\n",
        "\n",
        "# Define transformations\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor()  # Make sure this is the last step\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()  # Make sure this is the last step\n",
        "])\n",
        "\n",
        "# Paths to dataset directories\n",
        "train_dir = '/content/drive/My Drive/NYUDatathon/Train_data'\n",
        "test_dir = '/content/drive/My Drive/NYUDatathon/test_data'\n",
        "\n",
        "# Creating dataset instances with transformations\n",
        "train_dataset = ConstructionSiteDataset(train_dir, transform=train_transforms, device='cpu')\n",
        "test_dataset = ConstructionSiteDataset(test_dir, transform=test_transforms, device='cpu')\n",
        "\n",
        "# Print class counts from the train dataset\n",
        "print(\"Train dataset class counts:\", train_dataset.get_class_counts())\n",
        "\n",
        "# Calculate weights for each sample in the dataset\n",
        "class_weights = {cls_name: 1.0 / count for cls_name, count in train_dataset.get_class_counts().items()}\n",
        "sample_weights = [class_weights[_class] for _, _class in train_dataset.images]\n",
        "\n",
        "# Create the WeightedRandomSampler\n",
        "sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
        "\n",
        "# DataLoader using the sampler\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, sampler=sampler, shuffle=False)  # shuffle is False when using a sampler\n",
        "\n",
        "# DataLoader for the test dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
        "\n",
        "# Verifying one sample batch\n",
        "for images, labels in train_loader:\n",
        "    print(\"Batch images shape:\", images.shape)\n",
        "    print(\"Batch labels:\", labels)\n",
        "    break  # Only print the first batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icYHTHQ4IXzO",
        "outputId": "bf25ecec-5dfb-4b7d-821d-a37be228c449"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset class counts: {'underdeveloped': 16, 'ground_broken': 120, 'concrete_pad': 15, 'framing_up': 17, 'near_completion': 33}\n",
            "Batch images shape: torch.Size([10, 3, 224, 224])\n",
            "Batch labels: tensor([0, 3, 3, 3, 0, 1, 1, 2, 3, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple CNN model\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 32, 3, padding=1),  # Example layer\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(32 * 112 * 112, 5)  # Assuming the output size from Conv2d and pooling\n",
        ")\n",
        "model.to('cpu')  # Or 'cuda' if you have GPU support\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):  # Loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # Get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:  # Print every 10 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqCx5VnCJPw9",
        "outputId": "c19864ef-5d19-47a7-c536-1e3c47bda9a4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 10] loss: 17.815\n",
            "[1, 20] loss: 7.329\n",
            "[2, 10] loss: 4.551\n",
            "[2, 20] loss: 1.851\n",
            "[3, 10] loss: 1.866\n",
            "[3, 20] loss: 1.432\n",
            "[4, 10] loss: 1.351\n",
            "[4, 20] loss: 1.141\n",
            "[5, 10] loss: 1.019\n",
            "[5, 20] loss: 0.942\n",
            "[6, 10] loss: 0.941\n",
            "[6, 20] loss: 0.934\n",
            "[7, 10] loss: 0.945\n",
            "[7, 20] loss: 0.763\n",
            "[8, 10] loss: 0.805\n",
            "[8, 20] loss: 0.715\n",
            "[9, 10] loss: 0.611\n",
            "[9, 20] loss: 0.666\n",
            "[10, 10] loss: 0.604\n",
            "[10, 20] loss: 0.563\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # No gradients needed\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the model on the test images: {accuracy:.2f}%')\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "_5BaLT8VJfzn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_accuracy = evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7QqDg6OKGsw",
        "outputId": "5b8d6c56-3afd-4894-a5f6-cfbcb2a49ed1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test images: 67.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def detailed_evaluation(model, test_loader):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    # Print detailed classification report\n",
        "    print(classification_report(all_labels, all_predictions, target_names=['underdeveloped', 'ground_broken', 'concrete_pad', 'framing_up', 'near_completion']))\n",
        "\n",
        "# Example call to this function\n",
        "detailed_evaluation(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIyb_pW9KI9M",
        "outputId": "798735de-eb3e-4087-e1aa-cbd877a93bfa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            " underdeveloped       0.48      1.00      0.65        11\n",
            "  ground_broken       1.00      0.38      0.56        26\n",
            "   concrete_pad       0.89      1.00      0.94         8\n",
            "     framing_up       0.33      0.50      0.40         2\n",
            "near_completion       0.70      0.88      0.78         8\n",
            "\n",
            "       accuracy                           0.67        55\n",
            "      macro avg       0.68      0.75      0.66        55\n",
            "   weighted avg       0.81      0.67      0.66        55\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Final Model"
      ],
      "metadata": {
        "id": "rLZUgnVlWl_u"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#taking into consideration the limitations of the older model, we enhance the cnn\n",
        "class ComplexCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ComplexCNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(256 * 28 * 28, 1024)  # Adjust the size according to your final feature map dimensions\n",
        "        self.fc2 = nn.Linear(1024, 5)  # Output layer for 5 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the features into a vector\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Model instantiation\n",
        "model = ComplexCNN()\n",
        "model.to('cpu')  # Use 'cuda' if GPU is available\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):  # Loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # Get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:  # Print every 10 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z82N5M7bKQoE",
        "outputId": "ffc94506-523e-4651-f6b8-a72299694e4c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 10] loss: 150.109\n",
            "[1, 20] loss: 127.345\n",
            "[2, 10] loss: 120.672\n",
            "[2, 20] loss: 88.520\n",
            "[3, 10] loss: 29.168\n",
            "[3, 20] loss: 26.536\n",
            "[4, 10] loss: 22.217\n",
            "[4, 20] loss: 15.994\n",
            "[5, 10] loss: 10.265\n",
            "[5, 20] loss: 11.023\n",
            "[6, 10] loss: 11.267\n",
            "[6, 20] loss: 10.409\n",
            "[7, 10] loss: 7.727\n",
            "[7, 20] loss: 9.665\n",
            "[8, 10] loss: 7.311\n",
            "[8, 20] loss: 5.311\n",
            "[9, 10] loss: 9.902\n",
            "[9, 20] loss: 10.038\n",
            "[10, 10] loss: 10.960\n",
            "[10, 20] loss: 10.620\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_accuracy = evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK-WK8X8LAgT",
        "outputId": "bdac5512-c584-4797-cad0-139a9e3d1241"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test images: 61.82%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def detailed_evaluation(model, test_loader):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    # Print detailed classification report\n",
        "    print(classification_report(all_labels, all_predictions, target_names=['underdeveloped', 'ground_broken', 'concrete_pad', 'framing_up', 'near_completion']))\n",
        "\n",
        "# Example call to this function\n",
        "detailed_evaluation(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CPo819yLJrW",
        "outputId": "9aa98d4c-9b39-4bbb-eb75-ead83d8a5f40"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            " underdeveloped       0.57      0.36      0.44        11\n",
            "  ground_broken       0.89      0.65      0.76        26\n",
            "   concrete_pad       0.36      0.62      0.45         8\n",
            "     framing_up       0.14      0.50      0.22         2\n",
            "near_completion       0.88      0.88      0.88         8\n",
            "\n",
            "       accuracy                           0.62        55\n",
            "      macro avg       0.57      0.60      0.55        55\n",
            "   weighted avg       0.72      0.62      0.65        55\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "apYyLbbJTgHN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}